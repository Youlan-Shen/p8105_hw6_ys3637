---
title: "p8105_hw6_ys3637"
author: "Youlan Shen"
date: "2022-12-03"
output: github_document
---

## Set up

```{r}
# library all packages that we need at the beginning
library(tidyverse)
library(modelr)
library(mgcv)

set.seed(1)

# default set up
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 2

First loads, tidies, and clean the data.

Create a city_state variable, omit 4 cities, and limit race to White and Black

```{r}
# read in data from CSV file
homicide_data <- read_csv("Data/homicide-data.csv")
# show the first several lines of the original data
homicide_data
# Create a city_state variable, omit 4 cities, and limit race to White and Black
homicide_data <- homicide_data %>% 
  janitor::clean_names() %>% 
  mutate(city_state = str_c(city, ", ", state),
         victim_age = as.numeric(victim_age)) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  filter(victim_race %in% c("White","Black"))
# show the first several lines of the cleaned data
homicide_data
```

For the city of Baltimore, MD, to fit a logistic regression on resolved and unresolved as the outcome, and victim age, sex and race as predictors.

First we mutate the disposition to a binary outcome and select predictors.

```{r}
# first mutate the disposition column and save the dataframe
baltimore_data <- 
  homicide_data %>% 
  filter(city_state == "Baltimore, MD") %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest")) %>% 
  select(resolved, victim_age, victim_sex, victim_race)
```

Using the baltimore_data, we fit a logistic regression on resolved. Then obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
# fit a logistic regression model on baltimore data
baltimore_glm <- 
  baltimore_data %>% 
  glm(resolved ~ victim_age + victim_sex + victim_race , data = ., family = binomial())
# obtain the estimate and CI for adjusted OR comparing male victims to female victims
baltimore_glm %>% 
  broom::tidy() %>% 
  mutate(adjusted_OR = exp(estimate),
         CI_OR_Lower = exp(confint(baltimore_glm, level = 0.95))[,1],
         CI_OR_Upper = exp(confint(baltimore_glm, level = 0.95))[,2]) %>%
  select(term, estimate, adjusted_OR, CI_OR_Lower, CI_OR_Upper) %>% 
  filter(term == "victim_sexMale") %>% 
  knitr::kable(digits = 3)
```

Then, run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims.

```{r}
# first mutate the disposition column and save the dataframe for the whole 
# dataset
homicide_data_glm <- 
  homicide_data %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest")) %>% 
  select(city_state, resolved, victim_age, victim_sex, victim_race)
# apply the glm to each city
glm_each_city <- homicide_data_glm %>% 
  nest(data = -city_state) %>% 
  mutate(glm_models = map(data, ~glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())),
         results = map(glm_models, broom::tidy)) %>% 
    select(city_state, results) %>% 
    unnest(cols = results) %>% 
    mutate(adjusted_OR = exp(estimate),
         CI_OR_Lower = exp(estimate - 1.96 * std.error),
         CI_OR_Upper = exp(estimate + 1.96 * std.error)) %>%
  select(city_state, term, estimate, adjusted_OR, CI_OR_Lower, CI_OR_Upper) %>%
  filter(term == "victim_sexMale")
# Show the result
glm_each_city  %>% 
  knitr::kable(digits = 3)
```

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r}
glm_each_city %>% 
  ggplot(aes(x = fct_reorder(city_state, adjusted_OR, max), y = adjusted_OR, color = city_state)) + 
  geom_errorbar(aes(ymin = CI_OR_Lower, ymax = CI_OR_Upper)) +
  labs(
    title = "Adjusted OR for Solving Homicides Male V.S. Female Victims for Each City",
    x = "City And State",
    y = "Adjected OR Male V.S. Female Victimes") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Form the plot, we can see that most of the adjusted ORs are lower than 1, which means in most of the cities, if the victim is female, then the case is harder to close(the suspect will be less likely to be caught), compared to the male victims. However, since there is also a large proportion of cities' OR confidence interval contains 1, we cannot confidently conclude that the cases for female victims are harder to close. For cities whose OR confidence intervals do not contain 1, we can conclude that we have evidence that the cases for female victims are harder to close than those for male victims.

## Problem 3

First load and clean the dataset.

```{r}
# read in data from CSV file
birthweight_data <- read_csv("Data/birthweight.csv")
# show the first several lines of the original data
birthweight_data
# clean the data
birthweight_data <- birthweight_data  %>% 
  janitor::clean_names() %>%  
  mutate(babysex = ifelse(babysex == 1, "male", "female"),
         mrace = ifelse(mrace == 1, "White", ifelse(mrace == 2, "Black", ifelse(mrace == 3, "Asian", ifelse(mrace == 4, "Ruerto Rican", "Other"))))) %>%
  mutate(babysex = fct_infreq(babysex),
         mrace = fct_infreq(mrace))
# show the first several lines of the cleaned data
birthweight_data
# check NAs
birthweight_data %>% 
  is.na() %>% 
  colSums() %>% 
  knitr::kable(col.names = c("Counts of NA"))
# write_csv(birthweight_data, "Data/birthweight2.csv") for check
```

There is no missing value for each column.

Propose a regression model for birthweight.

I will limit to variables that I think have lower colinearity, and are significant, and use step function with backwards direction to generate a model. 

Model 1:
bwt ~ babysex + blength + gaweeks + malform + momage + mrace + smoken + wtgain

I exclude bhead (which I think is related to blength), delwt, fincome, mheighth, ppbmi, ppwt (which I think is related to wtgain), frace, menarche, parity, pnumlbw, pnumgsa (which I think is less important here).

```{r}
m <- lm(bwt ~ babysex + blength + gaweeks + malform + momage + mrace + smoken + wtgain, data = birthweight_data)
# use step function with backwards direction to generate a model
m1 <- step(m, direction = "backward", trace = FALSE)
# R^2 stats
m1 %>% broom::glance()
# estimates and p values
m1 %>%
  broom::tidy() %>%
  select(term, estimate, p.value) %>% 
  knitr::kable(
    digits = 4
  )
```

After the step function with backwards direction, I generate a final model with 7 variables.

* Final Model 1:
bwt ~ babysex + blength + gaweeks + momage + mrace + smoken + wtgain

Then, show a plot of model residuals against fitted values.

```{r}
birthweight_data %>% 
  modelr::add_residuals(m1) %>% 
  modelr::add_predictions(m1)  %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  labs(
    title = "Model 1 Residuals Against Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) 
```

Using crossv_mc to compare the model 1 with other two models.

* Model 2 :
bwt ~ blength + gaweeks

* Model 3:
bwt ~ bhead * blength * babysex

First generate the CV dataframe.
```{r}
# select usable variables from the birthweight_data
birthweight <- birthweight_data %>% 
  select(bwt, babysex, bhead, blength, gaweeks, momage, mrace, smoken, wtgain)
# generate a cv dataframe at the beginning
cv_df =
  crossv_mc(birthweight, 30) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

Then fit 3 models to the generated CV dataframe.

```{r}
cv_df <-
  cv_df %>% 
  mutate(
    model_1  = map(train, ~lm(bwt ~ babysex + blength + gaweeks + momage + mrace + smoken + wtgain, data = .x)),
    model_2     = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_3 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_2    = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y)))
```

In the end, to plot the prediction error for 3 models.

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

From the plot, I can conclude that regardign to prediction error, model 3 is the best, since it has the lowest RMSEs. The speads of RMSEs in three models are similar, and model 1 has lower RMSE compared to model 2. So, if we only look at the prediction error, I could conclude that the model 3: bwt ~ bhead * blength * babysex is the best. In the meantiem, model 3 does not have lots of predictors, so it is a great model here.